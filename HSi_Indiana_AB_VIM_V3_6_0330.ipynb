{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Judyxyang/judyxyang/blob/master/HSi_Indiana_AB_VIM_V3_6_0330.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7adnkHUo9TBz"
      },
      "source": [
        "# HyperMamba Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3DW2ucDRG2Z",
        "outputId": "bcccc8d5-5f67-45cd-c21f-bf1f9d9653ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spectral\n",
            "  Downloading spectral-0.23.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mat73\n",
            "  Downloading mat73-0.62-py3-none-any.whl (19 kB)\n",
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spectral) (1.25.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from mat73) (3.9.0)\n",
            "Installing collected packages: spectral, einops, mat73\n",
            "Successfully installed einops-0.7.0 mat73-0.62 spectral-0.23.1\n"
          ]
        }
      ],
      "source": [
        "pip install spectral mat73  einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdrWyJhZVfBP"
      },
      "source": [
        "#0 Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zudwFZMrRDDC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "9b22c24a-494e-4935-be14-88e7fff1a9c3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'einops'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ac0cce96a725>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0meinops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'einops'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "\n",
        "from einops import rearrange\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from scipy import io\n",
        "import torch.utils.data\n",
        "import scipy.io as sio\n",
        "import mat73\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OATt9TAf90MC"
      },
      "source": [
        "# 1 Upload Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js-m8VucycI3",
        "outputId": "ccc0e0fe-2462-402f-f98c-79578e9cb3db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl1ragFAUqfw",
        "outputId": "82c3d734-8865-4000-e31a-b24aa76979e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autoencodermodel_indian_fused.pth\t       indian_nir_attn_best_model.pt\n",
            "Autoencodermodel_indian.pth\t\t       Indian_pines_corrected.mat\n",
            "Autoencodermodel_indian_sole.pth\t       Indian_pines_gt.mat\n",
            "DFTC2013_Fusion_Model.h5\t\t       indian_uva_attn_best_model.pt\n",
            "Indian_BS_Model.h5\t\t\t       Indian_vim_model_state_dict.pth\n",
            "indian_hsi_attn_best_model_full_bands_v1.0.pt  indian_vis_attn_best_model.pt\n",
            "indian_hsi_attn_best_model_full_bands_v2.0.pt  indian_vis_attn_best_model_v1.0.pt\n",
            "indian_hsi_attn_best_model_v1.0.pt\t       PaviaU_model_state_dict.pth\n",
            "/content/drive/MyDrive/A02_RemoteSensingData/IndianPines/Indian_pines_corrected.mat\n",
            "/content/drive/MyDrive/A02_RemoteSensingData/IndianPines/Indian_pines_gt.mat\n"
          ]
        }
      ],
      "source": [
        "# 1.1 Upload the HSI data from Google driver\n",
        "! ls '/content/drive/MyDrive/A02_RemoteSensingData/IndianPines/'\n",
        "!ls '/content/drive/MyDrive/A02_RemoteSensingData/IndianPines/Indian_pines_corrected.mat'\n",
        "!ls '/content/drive/MyDrive/A02_RemoteSensingData/IndianPines/Indian_pines_gt.mat'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BXmEJRaU6Zp",
        "outputId": "6f51ca88-50c4-49b6-c601-20c6f8a21082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_data shape: (145, 145, 200)\n",
            "gt_data.shape: (145, 145)\n"
          ]
        }
      ],
      "source": [
        "# path\n",
        "path ='/content/drive/MyDrive/A02_RemoteSensingData/IndianPines/'\n",
        "\n",
        "# Load hyperpsectral data\n",
        "hsi_data=sio.loadmat(path+'Indian_pines_corrected.mat')\n",
        "#print('hsi_data shape:', hsi_data.shape)\n",
        "gt_data=sio.loadmat(path+'Indian_pines_gt.mat')\n",
        "\n",
        "# Load hyperpsectral data\n",
        "hsi_data=sio.loadmat(path+'Indian_pines_corrected.mat')['indian_pines_corrected']\n",
        "print('hsi_data shape:', hsi_data.shape)\n",
        "\n",
        "#Load ground truth labels\n",
        "gt_data=sio.loadmat(path+'Indian_pines_gt.mat')['indian_pines_gt']\n",
        "print('gt_data.shape:', gt_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vVIqLhgvnQj"
      },
      "source": [
        "# 1 Model Building\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6TlJRoRw0z4"
      },
      "outputs": [],
      "source": [
        "# Configuration class\n",
        "class Config:\n",
        "    def __init__(self, in_channels, num_patches, kernel_size, patch_size, emb_size, dim, depth, heads, dim_head, mlp_ratio, num_classes, dropout, pos_emb_size, class_emb_size, stride, output_dim):  # Set default output_dim to 1\n",
        "        self.in_channels = in_channels\n",
        "        self.num_patches = num_patches\n",
        "        self.kernel_size = kernel_size\n",
        "        self.patch_size = patch_size\n",
        "        self.emb_size = emb_size\n",
        "        self.dim = dim\n",
        "        self.depth = depth\n",
        "        self.heads = heads\n",
        "        self.dim_head = dim_head\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout = dropout\n",
        "        self.pos_emb_size = pos_emb_size\n",
        "        self.class_emb_size = class_emb_size\n",
        "        self.stride = stride\n",
        "        self.output_dim = output_dim  # Ensure output_dim is a part of the config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 HSI_VIM"
      ],
      "metadata": {
        "id": "_uPP_U68SR6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 2.0 This involves reversing the input tensor for the backward path before applying the backward_conv1d operation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class HSIVimBlock(nn.Module):\n",
        "    def __init__(self, spatial_dim, num_bands, hidden_dim, output_dim, delta_param_init):\n",
        "        super(HSIVimBlock, self).__init__()\n",
        "        # Initialization with self.hidden_dim\n",
        "        self.spatial_dim = spatial_dim\n",
        "        self.num_bands = num_bands\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # LayerNorm is now expecting a flattened feature vector of Bands*H*W elements\n",
        "        self.norm = nn.LayerNorm(num_bands * spatial_dim * spatial_dim)\n",
        "\n",
        "        # Adjust linear layers according to the new input dimension\n",
        "        self.linear_x = nn.Linear(num_bands * spatial_dim * spatial_dim, hidden_dim)\n",
        "        self.linear_z = nn.Linear(num_bands * spatial_dim * spatial_dim, hidden_dim)\n",
        "\n",
        "        self.forward_conv1d = nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
        "        self.backward_conv1d = nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
        "\n",
        "        self.A = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "        self.B = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "        #self.C = nn.Parameter(torch.randn(output_dim, hidden_dim))\n",
        "        self.delta_param = nn.Parameter(torch.full((hidden_dim,), delta_param_init))\n",
        "\n",
        "        self.linear_forward = nn.Linear(hidden_dim, output_dim)\n",
        "        self.linear_backward = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Batch, H, W, Bands = x.shape  # Correct shape extraction assuming [Batch, Height, Width, Bands]\n",
        "\n",
        "        # Correctly reshape for LayerNorm to flatten all spatial and spectral information\n",
        "        x = x.reshape(Batch, -1)  # New shape: [Batch, Bands*H*W]\n",
        "\n",
        "        # Normalize across the flattened spatial-spectral data\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Projection to hidden dimensions\n",
        "        x_proj = self.linear_x(x)\n",
        "        z_proj = self.linear_z(x)\n",
        "\n",
        "        # Ensure correct reshaping for Conv1d compatibility\n",
        "        x_proj = x_proj.view(Batch, self.hidden_dim, -1)\n",
        "        z_proj = z_proj.view(Batch, self.hidden_dim, -1)\n",
        "\n",
        "        # Reverse z_proj for the backward path\n",
        "        z_proj_reversed = torch.flip(z_proj, dims=[-1])\n",
        "\n",
        "        # Bidirectional Conv1d processing using reversed input for the backward path\n",
        "        x_forward = F.silu(self.forward_conv1d(x_proj))\n",
        "        x_backward = F.silu(self.backward_conv1d(z_proj_reversed))\n",
        "\n",
        "        # Apply delta parameter correctly\n",
        "        delta_expanded = self.delta_param.unsqueeze(0).unsqueeze(2)  # Correct shape for broadcasting\n",
        "\n",
        "        # SSM processing with delta applied, using the original and reversed inputs for forward and backward paths respectively\n",
        "        forward_ssm_output = torch.tanh(self.forward_conv1d(x_proj) + self.A * delta_expanded)\n",
        "        backward_ssm_output = torch.tanh(self.backward_conv1d(z_proj_reversed) + self.B * delta_expanded)\n",
        "\n",
        "        # Combine forward and backward outputs into a single representation\n",
        "        forward_reduced = forward_ssm_output.mean(dim=2)\n",
        "        backward_reduced = backward_ssm_output.mean(dim=2)\n",
        "\n",
        "        # Combine the reduced forward and backward paths\n",
        "        y_forward = self.linear_forward(forward_reduced)\n",
        "        y_backward = self.linear_backward(backward_reduced)\n",
        "\n",
        "        # Element-wise sum of forward and backward outputs\n",
        "        y_combined = y_forward + y_backward\n",
        "\n",
        "        # Return the combined output\n",
        "        return y_combined"
      ],
      "metadata": {
        "id": "cIdxqtFgSYpV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnARddoOWdDk"
      },
      "source": [
        "### 1.2 SpatialFeature Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDkx3vqVoVWf"
      },
      "outputs": [],
      "source": [
        "# New version\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SpatialFeatureProcessing(nn.Module):\n",
        "    def __init__(self, input_channels):\n",
        "        super(SpatialFeatureProcessing, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            # First convolutional layer with dilation rate of 1 (standard convolution)\n",
        "            nn.Conv2d(in_channels=input_channels, out_channels=256, kernel_size=(3, 3), padding=1, dilation=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            # Second convolutional layer with a higher dilation rate to increase the receptive field\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3, 3), padding=2, dilation=2),  # Note the increased padding to maintain the spatial dimensions\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512)\n",
        "        )\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Adding global average pooling\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.global_avg_pool(x)  # Apply global average pooling\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten all dimensions except batch\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7IbGUpqWwbo"
      },
      "source": [
        "### 1.3 Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCVCyDOHf0ol"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_features, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(in_features=in_features, out_features=1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(in_features=1024, out_features=num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc_layers(x)\n",
        "        # Remove softmax here if you're using a loss function that includes it, such as nn.CrossEntropyLoss\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-iDoAegAj8D"
      },
      "source": [
        "### 1.4 Integrated into Main Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbvWBj6CjoQm"
      },
      "outputs": [],
      "source": [
        "class HSIClassificationMambaModel(nn.Module):\n",
        "    def __init__(self, spatial_dim, num_bands, hidden_dim, output_dim, delta_param_init, num_classes):\n",
        "        super(HSIClassificationMambaModel, self).__init__()\n",
        "        self.vim_block = HSIVimBlock(spatial_dim, num_bands, hidden_dim, output_dim, delta_param_init)\n",
        "        self.output_dim = output_dim  # Save output_dim as an attribute of the class\n",
        "\n",
        "        # Initialize SpatialFeatureProcessing and Classifier here\n",
        "        # Adjusted to pass 'output_dim' as 'input_channels' to SpatialFeatureProcessing\n",
        "        self.spatial_processing = SpatialFeatureProcessing(input_channels=output_dim)\n",
        "        # Assuming the output of SpatialFeatureProcessing matches the in_features expected by Classifier\n",
        "        self.classifier = Classifier(in_features=512, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vim_block(x)\n",
        "        # This is a placeholder. Actual reshaping depends on the output of HSIVimBlock and the input expectation of SpatialFeatureProcessing\n",
        "        x = x.view(-1, self.output_dim, 1, 1)  # Reshape to include spatial dimensions if needed\n",
        "        x = self.spatial_processing(x)\n",
        "\n",
        "        # Flatten the output from spatial processing if it's not already flat\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YPTp6kBEwgr"
      },
      "source": [
        "### 3.9 Instance the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjaOisQ-e9vP",
        "outputId": "11d31d7f-f7db-4ecd-9666-a75ec58e9359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HSIClassificationMambaModel(\n",
            "  (vim_block): HSIVimBlock(\n",
            "    (norm): LayerNorm((16200,), eps=1e-05, elementwise_affine=True)\n",
            "    (linear_x): Linear(in_features=16200, out_features=256, bias=True)\n",
            "    (linear_z): Linear(in_features=16200, out_features=256, bias=True)\n",
            "    (forward_conv1d): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    (backward_conv1d): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    (linear_forward): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (linear_backward): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (classifier): Classifier(\n",
            "    (fc_layers): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=1024, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.4, inplace=False)\n",
            "      (3): Linear(in_features=1024, out_features=16, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Instantiate the model\n",
        "model = HSIClassificationMambaModel(\n",
        "    spatial_dim=7,\n",
        "    num_bands=200,\n",
        "    hidden_dim=256,\n",
        "    output_dim=128,  # Make sure this matches the actual output of your HSIVimBlock\n",
        "    delta_param_init=0.01,\n",
        "    num_classes=16\n",
        ")\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hR7ELgGwo9R"
      },
      "source": [
        "### Training Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1mEtukizVr-",
        "outputId": "f5980467-338e-46d1-bdf5-c9f6c5b067ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: {'class_name': 'Corn-notil1', 'total_samples': 1434, 'training_samples': 50, 'test_samples': 1384}, 2: {'class_name': 'Corn-mintill', 'total_samples': 834, 'training_samples': 50, 'test_samples': 784}, 3: {'class_name': 'Corn', 'total_samples': 234, 'training_samples': 50, 'test_samples': 184}, 4: {'class_name': 'Grass-pasture', 'total_samples': 497, 'training_samples': 50, 'test_samples': 447}, 5: {'class_name': 'Grass-trees', 'total_samples': 747, 'training_samples': 50, 'test_samples': 697}, 6: {'class_name': 'hay Windrowed', 'total_samples': 489, 'training_samples': 50, 'test_samples': 439}, 7: {'class_name': 'Soybean nNti11', 'total_samples': 968, 'training_samples': 50, 'test_samples': 918}, 8: {'class_name': 'Soybean-Minitill', 'total_samples': 2468, 'training_samples': 50, 'test_samples': 2418}, 9: {'class_name': 'Soybean Clean', 'total_samples': 614, 'training_samples': 50, 'test_samples': 564}, 10: {'class_name': ' Wheat', 'total_samples': 212, 'training_samples': 50, 'test_samples': 162}, 11: {'class_name': ' Woods', 'total_samples': 1294, 'training_samples': 50, 'test_samples': 1244}, 12: {'class_name': 'Buildings Grass Tress Drives', 'total_samples': 380, 'training_samples': 50, 'test_samples': 330}, 13: {'class_name': 'Stone Steel Towers', 'total_samples': 95, 'training_samples': 50, 'test_samples': 45}, 14: {'class_name': 'Alfalfa', 'total_samples': 54, 'training_samples': 15, 'test_samples': 39}, 15: {'class_name': 'Grass-pasture-mowed', 'total_samples': 26, 'training_samples': 15, 'test_samples': 11}, 16: {'class_name': 'Oats', 'total_samples': 20, 'training_samples': 15, 'test_samples': 5}}\n"
          ]
        }
      ],
      "source": [
        "# Define the class information\n",
        "class_info = [\n",
        "    (1, \"Corn-notil1\", \"samples\", 1434, 'training_sample',50, 'test_sample', 1384 ),\n",
        "    (2, \"Corn-mintill\", \"samples\", 834,'training_sample',50, 'test_sample', 784 ),\n",
        "    (3, \"Corn\",\"samples\",234,'training_sample',50, 'test_sample', 184),\n",
        "    (4, \"Grass-pasture\",\"samples\",497,'training_sample',50, 'test_sample', 447),\n",
        "    (5, \"Grass-trees\",\"samples\", 747 ,'training_sample',50, 'test_sample', 697),\n",
        "    (6, \"hay Windrowed\",\"samples\",489,'training_sample',50, 'test_sample', 439),\n",
        "    (7, \"Soybean nNti11\",\"samples\",968,'training_sample',50, 'test_sample', 918),\n",
        "    (8, \"Soybean-Minitill\",\"samples\",2468,'training_sample',50, 'test_sample', 2418),\n",
        "    (9, \"Soybean Clean\", \"samples\",614,'training_sample',50, 'test_sample', 564),\n",
        "    (10, \" Wheat\", \"samples\",212 ,'training_sample',50, 'test_sample', 162),\n",
        "    (11, \" Woods\",\"samples\",1294,'training_sample',50, 'test_sample', 1244),\n",
        "    (12, \"Buildings Grass Tress Drives\", \"samples\",380,'training_sample',50, 'test_sample', 330),\n",
        "    (13, \"Stone Steel Towers\",\"samples\",95,'training_sample',50, 'test_sample', 45),\n",
        "    (14, \"Alfalfa\", \"samples\",54 ,'training_sample',15, 'test_sample', 39),\n",
        "    (15, \"Grass-pasture-mowed\",\"samples\",26,'training_sample',15, 'test_sample', 11),\n",
        "    (16, \"Oats\", \"samples\",20,'training_sample',15, 'test_sample', 5)\n",
        "]\n",
        "\n",
        "# Adjusting comprehension to correctly unpack the tuples\n",
        "class_dict = {class_number: {\"class_name\": class_name,\n",
        "                             \"total_samples\": total_samples,\n",
        "                             \"training_samples\": training_samples,\n",
        "                             \"test_samples\": test_samples}\n",
        "              for class_number, class_name, _, total_samples, _, training_samples, _, test_samples in class_info}\n",
        "\n",
        "\n",
        "print(class_dict)\n",
        "\n",
        "# # Create a dictionary to store class number, class name, and class samples\n",
        "# class_dict = {class_number: {\"class_name\": class_name, \"samples\": samples,'training_sample':training_sample} for class_number, class_name, samples in class_info}\n",
        "\n",
        "# print(class_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9615e24",
        "outputId": "6262d92a-955e-4695-e4c2-06c84457f25e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_samples shape: (10366, 9, 9, 200)\n",
            "labels shape: (10366,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define patch size and stride\n",
        "patch_size = 9\n",
        "stride = 1\n",
        "\n",
        "# Create an empty list to store patches and labels\n",
        "patches = []\n",
        "labels = []\n",
        "\n",
        "# Initialize a dictionary to store class count\n",
        "class_count = {i: 0 for i in class_dict.keys()}\n",
        "\n",
        "# Function to check if all classes have the required number of samples\n",
        "def all_classes_completed(class_count, class_dict):\n",
        "    return all(class_count[class_num] == class_dict[class_num][\"total_samples\"] for class_num in class_dict.keys())\n",
        "\n",
        "while not all_classes_completed(class_count, class_dict):\n",
        "    # Loop through the ground truth data\n",
        "    for label in class_dict.keys():\n",
        "        # Get the coordinates of the ground truth pixels\n",
        "        coords = np.argwhere(gt_data == label)\n",
        "\n",
        "        # Shuffle the coordinates to randomize the patch extraction\n",
        "        np.random.shuffle(coords)\n",
        "\n",
        "        for coord in coords:\n",
        "            i, j = coord\n",
        "            # Calculate the patch indices\n",
        "            i_start, i_end = i - patch_size // 2, i + patch_size // 2 + 1\n",
        "            j_start, j_end = j - patch_size // 2, j + patch_size // 2 + 1\n",
        "\n",
        "            # Check if the indices are within the bounds of the HSI data\n",
        "            if i_start >= 0 and i_end <= hsi_data.shape[0] and j_start >= 0 and j_end <= hsi_data.shape[1]:\n",
        "                # Extract the patch\n",
        "                patch = hsi_data[i_start:i_end, j_start:j_end, :]\n",
        "\n",
        "                # If the class count is less than the required samples\n",
        "                if class_count[label] < class_dict[label][\"total_samples\"]:\n",
        "                    # Append the patch and its label to the list\n",
        "                    patches.append(patch)\n",
        "                    labels.append(label)\n",
        "                    class_count[label] += 1\n",
        "\n",
        "                    # If all classes have the required number of samples, exit the loop\n",
        "                    if all_classes_completed(class_count, class_dict):\n",
        "                        break\n",
        "\n",
        "# Convert the list of patches and labels into arrays\n",
        "hsi_samples = np.array(patches)\n",
        "labels = np.array(labels)\n",
        "print('hsi_samples shape:', hsi_samples.shape)\n",
        "print('labels shape:', labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training_samples_dict based on class_dict\n",
        "training_samples_dict = {class_num: class_info[\"training_samples\"] for class_num, class_info in class_dict.items()}\n",
        "\n",
        "# Assuming `hsi_samples`, `lidar_samples`, and `labels` have been previously defined\n",
        "# Convert the list of patches and labels into arrays if they aren't already\n",
        "hsi_samples = np.array(hsi_samples)\n",
        "\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Create lists to store training and test samples and labels\n",
        "hsi_training_samples, lidar_training_samples, training_labels = [], [], []\n",
        "hsi_test_samples, lidar_test_samples, test_labels = [], [], []\n",
        "\n",
        "# Split samples into training and test sets based on the desired number of training samples\n",
        "for label, train_samples in training_samples_dict.items():\n",
        "    # Get indices of the current class\n",
        "    class_indices = np.where(labels == label)[0]\n",
        "\n",
        "    # Randomly shuffle the indices\n",
        "    np.random.shuffle(class_indices)\n",
        "\n",
        "    # Split the indices into training and test set indices\n",
        "    train_indices = class_indices[:train_samples]\n",
        "    test_indices = class_indices[train_samples:]\n",
        "\n",
        "    # Add training samples and labels for the current class\n",
        "    hsi_training_samples.extend(hsi_samples[train_indices])\n",
        "\n",
        "    training_labels.extend(labels[train_indices])\n",
        "\n",
        "    # Add test samples and labels for the current class\n",
        "    hsi_test_samples.extend(hsi_samples[test_indices])\n",
        "\n",
        "    test_labels.extend(labels[test_indices])\n",
        "\n",
        "# Convert lists back to numpy arrays\n",
        "hsi_training_samples = np.array(hsi_training_samples)\n",
        "\n",
        "training_labels = np.array(training_labels)\n",
        "\n",
        "hsi_test_samples = np.array(hsi_test_samples)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Print shapes to verify\n",
        "print('hsi_training_samples shape:', hsi_training_samples.shape)\n",
        "print('training_labels shape:', training_labels.shape)\n",
        "print('hsi_test_samples shape:', hsi_test_samples.shape)\n",
        "print('test_labels shape:', test_labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmuZG2CQ_tkB",
        "outputId": "c9a47492-514e-471d-ad30-d6a17870c759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_training_samples shape: (695, 9, 9, 200)\n",
            "training_labels shape: (695,)\n",
            "hsi_test_samples shape: (9671, 9, 9, 200)\n",
            "test_labels shape: (9671,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upT4F_PuVabo"
      },
      "source": [
        "###2.1 Augmentation Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,  y_train=hsi_training_samples,training_labels\n",
        "X_test, y_test=hsi_test_samples, test_labels"
      ],
      "metadata": {
        "id": "0dbyMIKwAS3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIIinGlVVc6F",
        "outputId": "6ee74bd7-7e4e-43a4-b798-40f04a32dd5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented HSI training samples shape: (4170, 9, 9, 200)\n",
            "Augmented training labels shape: (4170,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import rotate\n",
        "\n",
        "def augment_training_data(hsi_training_data,  training_labels, rotations=[45, 90, 135], flip_up_down=True, flip_left_right=True):\n",
        "    augmented_hsi = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    for hsi,label in zip(hsi_training_data,  training_labels):\n",
        "        # Original data\n",
        "        augmented_hsi.append(hsi)\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "        # Rotations\n",
        "        for angle in rotations:\n",
        "            hsi_rotated = rotate(hsi, angle, axes=(0, 1), reshape=False, mode='nearest')\n",
        "            augmented_hsi.append(hsi_rotated)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "        # Flip up-down\n",
        "        if flip_up_down:\n",
        "            hsi_flipped_ud = np.flipud(hsi)\n",
        "\n",
        "            augmented_hsi.append(hsi_flipped_ud)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "        # Flip left-right\n",
        "        if flip_left_right:\n",
        "            hsi_flipped_lr = np.fliplr(hsi)\n",
        "\n",
        "            augmented_hsi.append(hsi_flipped_lr)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "    return np.array(augmented_hsi), np.array(augmented_labels)\n",
        "\n",
        "# Augmenting the training samples\n",
        "augmented_hsi_training_samples,  augmented_training_labels = augment_training_data(X_train,  y_train)\n",
        "\n",
        "# Print shapes to verify the augmented training data\n",
        "print('Augmented HSI training samples shape:', augmented_hsi_training_samples.shape)\n",
        "print('Augmented training labels shape:', augmented_training_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aH8AYC73RUK",
        "outputId": "ec43afef-7b95-41f7-d968-6cc313e8b865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (4170, 9, 9, 200)\n",
            "X_val shape: (209, 9, 9, 200)\n",
            "X_test shape: (9671, 9, 9, 200)\n",
            "y_train shape: (4170,)\n",
            "y_val shape: (209,)\n",
            "y_test shape: (9671,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the augmented training data into training, validationsets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "     augmented_hsi_training_samples, augmented_training_labels, test_size=0.05, random_state=42, stratify=augmented_training_labels\n",
        " )\n",
        "X_train=augmented_hsi_training_samples\n",
        "y_train=augmented_training_labels\n",
        "X_test=X_test\n",
        "y_test=y_test\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_val shape:', X_val.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_val shape:', y_val.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "\n",
        "\n",
        "# Convert the splitted datasets to tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train.astype(np.float32)), torch.tensor(y_train).long())\n",
        "val_dataset = TensorDataset(torch.tensor(X_val.astype(np.float32)), torch.tensor(y_val).long())\n",
        "test_dataset = TensorDataset(torch.tensor(X_test.astype(np.float32)), torch.tensor(y_test).long())\n",
        "\n",
        "# Create DataLoader instances for training, validation, and testing\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtM-XdHTizW-"
      },
      "source": [
        "# 5.0 Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VFoCtttkXQL",
        "outputId": "7f723c58-fde6-45cd-8c9c-0f6f079b1ae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Memory Usage: 3251.38 MB\n"
          ]
        }
      ],
      "source": [
        "# import psutil\n",
        "# import os\n",
        "\n",
        "# # Function to get current process memory usage\n",
        "# def get_memory_usage():\n",
        "#     process = psutil.Process(os.getpid())\n",
        "#     return process.memory_info().rss / (1024 * 1024)  # Convert bytes to MB\n",
        "\n",
        "# initial_memory = get_memory_usage()\n",
        "# print(f\"Initial Memory Usage: {initial_memory:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhsKBbhgvrUy"
      },
      "outputs": [],
      "source": [
        "# Before the training loop, to record the initial memory usage (GPU)\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats()  # Reset peak memory stats at the start\n",
        "    initial_memory = torch.cuda.memory_allocated()\n",
        "    print(f\"Initial Memory Allocated: {initial_memory / 1e6} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Model in GPU\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "import time  # Step 1: Import the time module\n",
        "\n",
        "model = HSIClassificationMambaModel(\n",
        "    spatial_dim=1, num_bands=144, hidden_dim=256, output_dim=128, delta_param_init=0.01, num_classes=15\n",
        ").cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "\n",
        "epochs = 50\n",
        "best_val_loss = float('inf')\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "patience = 10\n",
        "\n",
        "start_time = time.time()  # Step 2: Record the start time\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.cuda(), labels.cuda() # Move the data into CUDA\n",
        "        optimizer.zero_grad()\n",
        "        labels -= 1\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda() # Move the data into CUDA\n",
        "            outputs = model(inputs)\n",
        "            labels -= 1\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_running_loss += loss.item()\n",
        "\n",
        "        epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
        "\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        print(f'Validation Loss Decreased({best_val_loss:.6f}--->{epoch_val_loss:.6f}) \\t Saving The Model')\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        no_improve_epochs = 0\n",
        "    else:\n",
        "        no_improve_epochs += 1\n",
        "\n",
        "    if no_improve_epochs > patience:\n",
        "        print('Early stopping!')\n",
        "        model.load_state_dict(best_model_wts)\n",
        "        break\n",
        "\n",
        "end_time = time.time()  # Step 3: Record the end time\n",
        "total_time = end_time - start_time  # Step 4: Calculate total training time\n",
        "\n",
        "print(f'Finished training. Total training time: {total_time:.2f} seconds')  # Print the total training time"
      ],
      "metadata": {
        "id": "GuDXjMa1aaPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before the training loop, to record the initial memory usage (GPU)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats()  # Reset peak memory stats at the start\n",
        "    initial_memory = torch.cuda.memory_allocated()\n",
        "    print(f\"Initial Memory Allocated: {initial_memory / 1e6} MB\")"
      ],
      "metadata": {
        "id": "5qbDQzgyalpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IPmnY75v8cq",
        "outputId": "21ea0e96-c760-49f3-e52f-071889fedf81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Loss: 0.0821, Val Loss: 0.0813\n",
            "Validation Loss Decreased(inf--->0.081340) \t Saving The Model\n",
            "Epoch [2/100], Train Loss: 0.0722, Val Loss: 0.0711\n",
            "Validation Loss Decreased(0.081340--->0.071110) \t Saving The Model\n",
            "Epoch [3/100], Train Loss: 0.0653, Val Loss: 0.0650\n",
            "Validation Loss Decreased(0.071110--->0.064980) \t Saving The Model\n",
            "Epoch [4/100], Train Loss: 0.0606, Val Loss: 0.0604\n",
            "Validation Loss Decreased(0.064980--->0.060418) \t Saving The Model\n",
            "Epoch [5/100], Train Loss: 0.0571, Val Loss: 0.0566\n",
            "Validation Loss Decreased(0.060418--->0.056646) \t Saving The Model\n",
            "Epoch [6/100], Train Loss: 0.0537, Val Loss: 0.0526\n",
            "Validation Loss Decreased(0.056646--->0.052572) \t Saving The Model\n",
            "Epoch [7/100], Train Loss: 0.0503, Val Loss: 0.0489\n",
            "Validation Loss Decreased(0.052572--->0.048901) \t Saving The Model\n",
            "Epoch [8/100], Train Loss: 0.0468, Val Loss: 0.0450\n",
            "Validation Loss Decreased(0.048901--->0.044970) \t Saving The Model\n",
            "Epoch [9/100], Train Loss: 0.0436, Val Loss: 0.0425\n",
            "Validation Loss Decreased(0.044970--->0.042502) \t Saving The Model\n",
            "Epoch [10/100], Train Loss: 0.0412, Val Loss: 0.0397\n",
            "Validation Loss Decreased(0.042502--->0.039694) \t Saving The Model\n",
            "Epoch [11/100], Train Loss: 0.0390, Val Loss: 0.0375\n",
            "Validation Loss Decreased(0.039694--->0.037472) \t Saving The Model\n",
            "Epoch [12/100], Train Loss: 0.0372, Val Loss: 0.0354\n",
            "Validation Loss Decreased(0.037472--->0.035361) \t Saving The Model\n",
            "Epoch [13/100], Train Loss: 0.0354, Val Loss: 0.0336\n",
            "Validation Loss Decreased(0.035361--->0.033555) \t Saving The Model\n",
            "Epoch [14/100], Train Loss: 0.0337, Val Loss: 0.0321\n",
            "Validation Loss Decreased(0.033555--->0.032108) \t Saving The Model\n",
            "Epoch [15/100], Train Loss: 0.0323, Val Loss: 0.0305\n",
            "Validation Loss Decreased(0.032108--->0.030450) \t Saving The Model\n",
            "Epoch [16/100], Train Loss: 0.0308, Val Loss: 0.0291\n",
            "Validation Loss Decreased(0.030450--->0.029144) \t Saving The Model\n",
            "Epoch [17/100], Train Loss: 0.0297, Val Loss: 0.0276\n",
            "Validation Loss Decreased(0.029144--->0.027595) \t Saving The Model\n",
            "Epoch [18/100], Train Loss: 0.0286, Val Loss: 0.0263\n",
            "Validation Loss Decreased(0.027595--->0.026270) \t Saving The Model\n",
            "Epoch [19/100], Train Loss: 0.0275, Val Loss: 0.0252\n",
            "Validation Loss Decreased(0.026270--->0.025210) \t Saving The Model\n",
            "Epoch [20/100], Train Loss: 0.0266, Val Loss: 0.0245\n",
            "Validation Loss Decreased(0.025210--->0.024465) \t Saving The Model\n",
            "Epoch [21/100], Train Loss: 0.0259, Val Loss: 0.0237\n",
            "Validation Loss Decreased(0.024465--->0.023674) \t Saving The Model\n",
            "Epoch [22/100], Train Loss: 0.0248, Val Loss: 0.0225\n",
            "Validation Loss Decreased(0.023674--->0.022521) \t Saving The Model\n",
            "Epoch [23/100], Train Loss: 0.0239, Val Loss: 0.0212\n",
            "Validation Loss Decreased(0.022521--->0.021204) \t Saving The Model\n",
            "Epoch [24/100], Train Loss: 0.0232, Val Loss: 0.0208\n",
            "Validation Loss Decreased(0.021204--->0.020772) \t Saving The Model\n",
            "Epoch [25/100], Train Loss: 0.0226, Val Loss: 0.0200\n",
            "Validation Loss Decreased(0.020772--->0.020040) \t Saving The Model\n",
            "Epoch [26/100], Train Loss: 0.0217, Val Loss: 0.0188\n",
            "Validation Loss Decreased(0.020040--->0.018779) \t Saving The Model\n",
            "Epoch [27/100], Train Loss: 0.0211, Val Loss: 0.0190\n",
            "Epoch [28/100], Train Loss: 0.0207, Val Loss: 0.0176\n",
            "Validation Loss Decreased(0.018779--->0.017578) \t Saving The Model\n",
            "Epoch [29/100], Train Loss: 0.0203, Val Loss: 0.0169\n",
            "Validation Loss Decreased(0.017578--->0.016889) \t Saving The Model\n",
            "Epoch [30/100], Train Loss: 0.0192, Val Loss: 0.0163\n",
            "Validation Loss Decreased(0.016889--->0.016324) \t Saving The Model\n",
            "Epoch [31/100], Train Loss: 0.0189, Val Loss: 0.0159\n",
            "Validation Loss Decreased(0.016324--->0.015921) \t Saving The Model\n",
            "Epoch [32/100], Train Loss: 0.0187, Val Loss: 0.0164\n",
            "Epoch [33/100], Train Loss: 0.0180, Val Loss: 0.0150\n",
            "Validation Loss Decreased(0.015921--->0.015009) \t Saving The Model\n",
            "Epoch [34/100], Train Loss: 0.0174, Val Loss: 0.0146\n",
            "Validation Loss Decreased(0.015009--->0.014625) \t Saving The Model\n",
            "Epoch [35/100], Train Loss: 0.0171, Val Loss: 0.0148\n",
            "Epoch [36/100], Train Loss: 0.0168, Val Loss: 0.0148\n",
            "Epoch [37/100], Train Loss: 0.0166, Val Loss: 0.0141\n",
            "Validation Loss Decreased(0.014625--->0.014104) \t Saving The Model\n",
            "Epoch [38/100], Train Loss: 0.0159, Val Loss: 0.0127\n",
            "Validation Loss Decreased(0.014104--->0.012738) \t Saving The Model\n",
            "Epoch [39/100], Train Loss: 0.0154, Val Loss: 0.0137\n",
            "Epoch [40/100], Train Loss: 0.0153, Val Loss: 0.0122\n",
            "Validation Loss Decreased(0.012738--->0.012240) \t Saving The Model\n",
            "Epoch [41/100], Train Loss: 0.0146, Val Loss: 0.0117\n",
            "Validation Loss Decreased(0.012240--->0.011739) \t Saving The Model\n",
            "Epoch [42/100], Train Loss: 0.0144, Val Loss: 0.0113\n",
            "Validation Loss Decreased(0.011739--->0.011304) \t Saving The Model\n",
            "Epoch [43/100], Train Loss: 0.0140, Val Loss: 0.0124\n",
            "Epoch [44/100], Train Loss: 0.0139, Val Loss: 0.0113\n",
            "Validation Loss Decreased(0.011304--->0.011254) \t Saving The Model\n",
            "Epoch [45/100], Train Loss: 0.0133, Val Loss: 0.0110\n",
            "Validation Loss Decreased(0.011254--->0.011041) \t Saving The Model\n",
            "Epoch [46/100], Train Loss: 0.0130, Val Loss: 0.0105\n",
            "Validation Loss Decreased(0.011041--->0.010460) \t Saving The Model\n",
            "Epoch [47/100], Train Loss: 0.0130, Val Loss: 0.0105\n",
            "Epoch [48/100], Train Loss: 0.0127, Val Loss: 0.0102\n",
            "Validation Loss Decreased(0.010460--->0.010239) \t Saving The Model\n",
            "Epoch [49/100], Train Loss: 0.0124, Val Loss: 0.0096\n",
            "Validation Loss Decreased(0.010239--->0.009582) \t Saving The Model\n",
            "Epoch [50/100], Train Loss: 0.0120, Val Loss: 0.0093\n",
            "Validation Loss Decreased(0.009582--->0.009282) \t Saving The Model\n",
            "Epoch [51/100], Train Loss: 0.0121, Val Loss: 0.0086\n",
            "Validation Loss Decreased(0.009282--->0.008620) \t Saving The Model\n",
            "Epoch [52/100], Train Loss: 0.0116, Val Loss: 0.0083\n",
            "Validation Loss Decreased(0.008620--->0.008332) \t Saving The Model\n",
            "Epoch [53/100], Train Loss: 0.0112, Val Loss: 0.0090\n",
            "Epoch [54/100], Train Loss: 0.0111, Val Loss: 0.0082\n",
            "Validation Loss Decreased(0.008332--->0.008228) \t Saving The Model\n",
            "Epoch [55/100], Train Loss: 0.0108, Val Loss: 0.0078\n",
            "Validation Loss Decreased(0.008228--->0.007814) \t Saving The Model\n",
            "Epoch [56/100], Train Loss: 0.0107, Val Loss: 0.0074\n",
            "Validation Loss Decreased(0.007814--->0.007395) \t Saving The Model\n",
            "Epoch [57/100], Train Loss: 0.0102, Val Loss: 0.0086\n",
            "Epoch [58/100], Train Loss: 0.0101, Val Loss: 0.0074\n",
            "Epoch [59/100], Train Loss: 0.0103, Val Loss: 0.0066\n",
            "Validation Loss Decreased(0.007395--->0.006614) \t Saving The Model\n",
            "Epoch [60/100], Train Loss: 0.0094, Val Loss: 0.0079\n",
            "Epoch [61/100], Train Loss: 0.0096, Val Loss: 0.0063\n",
            "Validation Loss Decreased(0.006614--->0.006282) \t Saving The Model\n",
            "Epoch [62/100], Train Loss: 0.0094, Val Loss: 0.0065\n",
            "Epoch [63/100], Train Loss: 0.0090, Val Loss: 0.0063\n",
            "Validation Loss Decreased(0.006282--->0.006264) \t Saving The Model\n",
            "Epoch [64/100], Train Loss: 0.0091, Val Loss: 0.0059\n",
            "Validation Loss Decreased(0.006264--->0.005851) \t Saving The Model\n",
            "Epoch [65/100], Train Loss: 0.0087, Val Loss: 0.0059\n",
            "Epoch [66/100], Train Loss: 0.0086, Val Loss: 0.0062\n",
            "Epoch [67/100], Train Loss: 0.0082, Val Loss: 0.0058\n",
            "Validation Loss Decreased(0.005851--->0.005818) \t Saving The Model\n",
            "Epoch [68/100], Train Loss: 0.0083, Val Loss: 0.0064\n",
            "Epoch [69/100], Train Loss: 0.0081, Val Loss: 0.0055\n",
            "Validation Loss Decreased(0.005818--->0.005531) \t Saving The Model\n",
            "Epoch [70/100], Train Loss: 0.0077, Val Loss: 0.0051\n",
            "Validation Loss Decreased(0.005531--->0.005105) \t Saving The Model\n",
            "Epoch [71/100], Train Loss: 0.0076, Val Loss: 0.0054\n",
            "Epoch [72/100], Train Loss: 0.0074, Val Loss: 0.0048\n",
            "Validation Loss Decreased(0.005105--->0.004844) \t Saving The Model\n",
            "Epoch [73/100], Train Loss: 0.0073, Val Loss: 0.0049\n",
            "Epoch [74/100], Train Loss: 0.0071, Val Loss: 0.0046\n",
            "Validation Loss Decreased(0.004844--->0.004579) \t Saving The Model\n",
            "Epoch [75/100], Train Loss: 0.0074, Val Loss: 0.0045\n",
            "Validation Loss Decreased(0.004579--->0.004542) \t Saving The Model\n",
            "Epoch [76/100], Train Loss: 0.0070, Val Loss: 0.0046\n",
            "Epoch [77/100], Train Loss: 0.0069, Val Loss: 0.0049\n",
            "Epoch [78/100], Train Loss: 0.0065, Val Loss: 0.0042\n",
            "Validation Loss Decreased(0.004542--->0.004234) \t Saving The Model\n",
            "Epoch [79/100], Train Loss: 0.0064, Val Loss: 0.0040\n",
            "Validation Loss Decreased(0.004234--->0.004007) \t Saving The Model\n",
            "Epoch [80/100], Train Loss: 0.0061, Val Loss: 0.0037\n",
            "Validation Loss Decreased(0.004007--->0.003732) \t Saving The Model\n",
            "Epoch [81/100], Train Loss: 0.0063, Val Loss: 0.0044\n",
            "Epoch [82/100], Train Loss: 0.0059, Val Loss: 0.0036\n",
            "Validation Loss Decreased(0.003732--->0.003640) \t Saving The Model\n",
            "Epoch [83/100], Train Loss: 0.0060, Val Loss: 0.0035\n",
            "Validation Loss Decreased(0.003640--->0.003515) \t Saving The Model\n",
            "Epoch [84/100], Train Loss: 0.0057, Val Loss: 0.0045\n",
            "Epoch [85/100], Train Loss: 0.0056, Val Loss: 0.0038\n",
            "Epoch [86/100], Train Loss: 0.0057, Val Loss: 0.0040\n",
            "Epoch [87/100], Train Loss: 0.0059, Val Loss: 0.0035\n",
            "Validation Loss Decreased(0.003515--->0.003467) \t Saving The Model\n",
            "Epoch [88/100], Train Loss: 0.0055, Val Loss: 0.0037\n",
            "Epoch [89/100], Train Loss: 0.0054, Val Loss: 0.0032\n",
            "Validation Loss Decreased(0.003467--->0.003246) \t Saving The Model\n",
            "Epoch [90/100], Train Loss: 0.0052, Val Loss: 0.0031\n",
            "Validation Loss Decreased(0.003246--->0.003086) \t Saving The Model\n",
            "Epoch [91/100], Train Loss: 0.0052, Val Loss: 0.0029\n",
            "Validation Loss Decreased(0.003086--->0.002937) \t Saving The Model\n",
            "Epoch [92/100], Train Loss: 0.0050, Val Loss: 0.0031\n",
            "Epoch [93/100], Train Loss: 0.0050, Val Loss: 0.0044\n",
            "Epoch [94/100], Train Loss: 0.0049, Val Loss: 0.0042\n",
            "Epoch [95/100], Train Loss: 0.0049, Val Loss: 0.0031\n",
            "Epoch [96/100], Train Loss: 0.0050, Val Loss: 0.0029\n",
            "Epoch [97/100], Train Loss: 0.0047, Val Loss: 0.0027\n",
            "Validation Loss Decreased(0.002937--->0.002687) \t Saving The Model\n",
            "Epoch [98/100], Train Loss: 0.0042, Val Loss: 0.0025\n",
            "Validation Loss Decreased(0.002687--->0.002538) \t Saving The Model\n",
            "Epoch [99/100], Train Loss: 0.0045, Val Loss: 0.0024\n",
            "Validation Loss Decreased(0.002538--->0.002359) \t Saving The Model\n",
            "Epoch [100/100], Train Loss: 0.0043, Val Loss: 0.0024\n",
            "Finished training. Total training time: 2320.34 seconds\n"
          ]
        }
      ],
      "source": [
        "# # Training on CPU\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import DataLoader\n",
        "# import copy\n",
        "# import time  # Step 1: Import the time module\n",
        "\n",
        "# model = HSIClassificationMambaModel(\n",
        "#     spatial_dim=9, num_bands=200, hidden_dim=256, output_dim=128, delta_param_init=0.001, num_classes=16\n",
        "# )\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "# #optimizer = optim.RMSprop(model.parameters(), lr=0.0001)  # RMSprop optimizer with learning rate 0.001\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min') # 2.5928\n",
        "\n",
        "# epochs = 100\n",
        "# best_val_loss = float('inf')\n",
        "# best_model_wts = copy.deepcopy(model.state_dict())\n",
        "# patience = 10  #  set patience to 10 epochs\n",
        "\n",
        "# start_time = time.time()  # Step 2: Record the start time\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     running_train_loss = 0.0  # Initialize running_train_loss at the start of each epoch\n",
        "#     for inputs, labels in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         labels -= 1  # Assuming labels need to be shifted\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         running_train_loss += loss.item()\n",
        "\n",
        "#     epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
        "\n",
        "#     model.eval()\n",
        "#     val_running_loss = 0.0\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, labels in val_loader:\n",
        "#             outputs = model(inputs)\n",
        "#             labels -= 1\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             val_running_loss += loss.item()\n",
        "\n",
        "#         epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
        "\n",
        "#     print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
        "\n",
        "#     if epoch_val_loss < best_val_loss:\n",
        "#         print(f'Validation Loss Decreased({best_val_loss:.6f}--->{epoch_val_loss:.6f}) \\t Saving The Model')\n",
        "#         best_val_loss = epoch_val_loss\n",
        "#         best_model_wts = copy.deepcopy(model.state_dict())\n",
        "#         no_improve_epochs = 0\n",
        "#     else:\n",
        "#         no_improve_epochs += 1\n",
        "\n",
        "#     if no_improve_epochs > patience:\n",
        "#         print('Early stopping!')\n",
        "#         model.load_state_dict(best_model_wts)\n",
        "#         break\n",
        "\n",
        "\n",
        "# end_time = time.time()  # Step 3: Record the end time\n",
        "# total_time = end_time - start_time  # Step 4: Calculate total training time\n",
        "\n",
        "# print(f'Finished training. Total training time: {total_time:.2f} seconds')  # Print the total training time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjHq_LfmkrBk",
        "outputId": "e7fe75ea-2590-4670-f982-1ce73a94eace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Memory Usage: 3269.28 MB\n",
            "Memory Used: 17.89 MB\n"
          ]
        }
      ],
      "source": [
        "# # Memory usage clacualtion\n",
        "# final_memory = get_memory_usage()\n",
        "# print(f\"Final Memory Usage: {final_memory:.2f} MB\")\n",
        "\n",
        "# memory_used = final_memory - initial_memory\n",
        "# print(f\"Memory Used: {memory_used:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC9h8grt9Kx_"
      },
      "outputs": [],
      "source": [
        "## If GPU USed\n",
        "# if torch.cuda.is_available():\n",
        "#     final_memory = torch.cuda.memory_allocated()\n",
        "#     peak_memory = torch.cuda.max_memory_allocated()\n",
        "#     print(f\"Final Memory Allocated: {final_memory / 1e6} MB\")\n",
        "#     print(f\"Peak Memory Allocated During Training: {peak_memory / 1e6} MB\")\n",
        "#     memory_used = final_memory - initial_memory\n",
        "#     print(f\"Memory Used: {memory_used / 1e6} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljUEozyPnONL"
      },
      "source": [
        "### 5.1  Save the modle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp56-lp2nQcJ"
      },
      "outputs": [],
      "source": [
        "# Assuming 'model' is your instance of HSIClassificationModel or any other model\n",
        "# and it's been trained\n",
        "torch.save(model.state_dict(),path+ 'p7_Indian_vim_model_state_dict.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SORIG-DiolCW",
        "outputId": "c01f7884-af84-4777-b355-70084bc58d9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to /content/drive/MyDrive/A02_RemoteSensingData/IndianPines/Indian_vim_model_state_dict.pth\n"
          ]
        }
      ],
      "source": [
        "# # Save the model\n",
        "# model_save_path = path+'Indian_vim_model_state_dict.pth'\n",
        "# torch.save(model.state_dict(), model_save_path)\n",
        "# print(f'Model saved to {model_save_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzWn7g5eZZFU"
      },
      "source": [
        "# 6.0 Upload the trained model and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnQAaFaO7Sjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f02cd287-3467-4358-8df2-49e5c21d71b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/A02_RemoteSensingData/IndianPines/Indian_vim_model_state_dict.pth\n",
            "Overall Accuracy (OA): 0.8787\n",
            "Average Accuracy (AA): 0.8727\n",
            "Kappa Coefficient: 0.8616\n",
            "Test time: 16.01 seconds\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
        "import time  # Import the time module for timing the test phase\n",
        "\n",
        "# Assuming 'model' is your instance of HSIClassificationModel or any other model\n",
        "# and it's been trained\n",
        "\n",
        "# Save the model\n",
        "model_save_path = path + 'p7_Indian_vim_model_state_dict.pth'\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f'Model saved to {model_save_path}')\n",
        "\n",
        "# Load the model (make sure to initialize the model architecture first)\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "model.to(device)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Store predictions and actual labels\n",
        "predictions = []\n",
        "actual_labels = []\n",
        "\n",
        "start_time = time.time()  # Start timing\n",
        "\n",
        "with torch.no_grad():\n",
        "    for hsi_patches, labels in test_loader:\n",
        "        # Move data to the appropriate device\n",
        "        hsi_patches = hsi_patches.to(device)\n",
        "        labels -= 1  # Adjust labels if necessary\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(hsi_patches)\n",
        "\n",
        "        # Get predictions\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "        actual_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "end_time = time.time()  # End timing\n",
        "test_time = end_time - start_time  # Calculate the test time\n",
        "\n",
        "# Optionally, calculate accuracy or other metrics using predictions and actual_labels\n",
        "\n",
        "# Convert lists to NumPy arrays for easier manipulation\n",
        "predictions_array = np.array(predictions)\n",
        "actual_labels_array = np.array(actual_labels)\n",
        "\n",
        "# Overall Accuracy\n",
        "oa = accuracy_score(actual_labels_array, predictions_array)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(actual_labels_array, predictions_array)\n",
        "# Calculate per-class accuracy from the confusion matrix\n",
        "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
        "# Average Accuracy\n",
        "aa = np.mean(class_accuracy)\n",
        "\n",
        "# Kappa Coefficient\n",
        "kappa = cohen_kappa_score(actual_labels_array, predictions_array)\n",
        "\n",
        "print(f'Overall Accuracy (OA): {oa:.4f}')\n",
        "print(f'Average Accuracy (AA): {aa:.4f}')\n",
        "print(f'Kappa Coefficient: {kappa:.4f}')\n",
        "print(f'Test time: {test_time:.2f} seconds')  # Print the test time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, acc in enumerate(class_accuracy): print(f'Class {i+1} Accuracy: {acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGcXhgv4FZ52",
        "outputId": "f7e6ecff-a5ae-42cd-dc5f-83c28f47bbb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 1 Accuracy: 1.0000\n",
            "Class 2 Accuracy: 0.4809\n",
            "Class 3 Accuracy: 0.8804\n",
            "Class 4 Accuracy: 0.9418\n",
            "Class 5 Accuracy: 0.9340\n",
            "Class 6 Accuracy: 0.9818\n",
            "Class 7 Accuracy: 1.0000\n",
            "Class 8 Accuracy: 0.9897\n",
            "Class 9 Accuracy: 1.0000\n",
            "Class 10 Accuracy: 0.7531\n",
            "Class 11 Accuracy: 0.5916\n",
            "Class 12 Accuracy: 0.7576\n",
            "Class 13 Accuracy: 1.0000\n",
            "Class 14 Accuracy: 0.7436\n",
            "Class 15 Accuracy: 0.9091\n",
            "Class 16 Accuracy: 1.0000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPwQmTo3VcyAn7+7SbJAax/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}