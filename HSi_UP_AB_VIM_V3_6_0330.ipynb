{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Judyxyang/judyxyang/blob/master/HSi_UP_AB_VIM_V3_6_0330.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7adnkHUo9TBz"
      },
      "source": [
        "# HyperMamba Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3DW2ucDRG2Z",
        "outputId": "a0e74c82-09c2-4b83-b947-a5e609fcad27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spectral\n",
            "  Downloading spectral-0.23.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mat73\n",
            "  Downloading mat73-0.62-py3-none-any.whl (19 kB)\n",
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spectral) (1.23.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from mat73) (3.9.0)\n",
            "Installing collected packages: spectral, einops, mat73\n",
            "Successfully installed einops-0.7.0 mat73-0.62 spectral-0.23.1\n"
          ]
        }
      ],
      "source": [
        "pip install spectral mat73  einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zudwFZMrRDDC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "\n",
        "from einops import rearrange\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from scipy import io\n",
        "import torch.utils.data\n",
        "import scipy.io as sio\n",
        "import mat73\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OATt9TAf90MC"
      },
      "source": [
        "# 0 Upload Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpwq4Yi-tgjs",
        "outputId": "1ac96038-789a-48c5-bd4a-c4c827aae0f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMin2GFJtorP",
        "outputId": "18fd6b91-80b5-468d-bbc3-984e90ab6cfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PaviaU_gt.mat  PaviaU_model_state_dict.pth\t PaviaU_p7_model_state_dict.pth\n",
            "PaviaU.mat     PaviaU_p7ab_model_state_dict.pth\n"
          ]
        }
      ],
      "source": [
        "! ls '/content/drive/MyDrive/A02_RemoteSensingData/PaviaU/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NMT_vf3t3Bg"
      },
      "outputs": [],
      "source": [
        "# # Define the path\n",
        "path='/content/drive/MyDrive/A02_RemoteSensingData/PaviaU/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMvtN-NsQard",
        "outputId": "01262516-8831-4100-8f4b-57dd77eee7e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PaviaU_hsi shape: (610, 340, 103)\n",
            "PaviaU_gt.shape: (610, 340)\n"
          ]
        }
      ],
      "source": [
        "PaviaU_hsi=sio.loadmat(path+'PaviaU.mat')['paviaU']\n",
        "print('PaviaU_hsi shape:', PaviaU_hsi.shape)\n",
        "\n",
        "#Load ground truth labels\n",
        "PaviaU_gt=sio.loadmat(path+'PaviaU_gt.mat')['paviaU_gt']\n",
        "print('PaviaU_gt.shape:', PaviaU_gt.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73Ss_28XaF6e"
      },
      "outputs": [],
      "source": [
        "class_info = [\n",
        "    (1, \"Asphalt\", 548, 6304, 6852),\n",
        "    (2, \"Meadows\", 540, 18146, 18686),\n",
        "    (3, \"Gravel\", 392, 1815, 2207),\n",
        "    (4, \"Trees\", 524, 2912, 3436),\n",
        "    (5, \"Metal Sheets\", 265, 1113, 1378),\n",
        "    (6, \"Bare Soil\", 532, 4572, 5104),\n",
        "    (7, \"Bitumen\", 375, 981, 1356),\n",
        "    (8, \"Bricks\", 514, 3364, 3878),\n",
        "    (9, \"Shadows\", 231, 795, 1026)\n",
        "]\n",
        "\n",
        "# Create a dictionary to store class number, class name, training samples, test samples, and total samples\n",
        "class_dict = {\n",
        "    class_number: {\n",
        "        \"class_name\": class_name,\n",
        "        \"training_samples\": training_samples,\n",
        "        \"test_samples\": test_samples,\n",
        "        \"total_samples\": total_samples\n",
        "    }\n",
        "    for class_number, class_name, training_samples, test_samples, total_samples in class_info\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImUtYfQePJ-4"
      },
      "source": [
        "### Supervised Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-Ys2Xu1bIfz",
        "outputId": "c238c951-140c-451f-ed92-b55367518df6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hsi_samples shape: (43923, 7, 7, 103)\n",
            "labels shape: (43923,)\n"
          ]
        }
      ],
      "source": [
        "# 2.2 Samples Extraction\n",
        "\n",
        "# # Create a mask with all class labels\n",
        "# mask = np.copy(gt_2013_data)\n",
        "\n",
        "# # Set the background class to 0\n",
        "# mask[mask == 0] = 0\n",
        "\n",
        "# Define patch size and stride\n",
        "patch_size = 7\n",
        "stride = 1\n",
        "\n",
        "# Create an empty list to store patches and labels\n",
        "hsi_samples = []\n",
        "labels = []\n",
        "\n",
        "# Initialize a dictionary to store class count\n",
        "class_count = {i: 0 for i in class_dict.keys()}\n",
        "\n",
        "# Function to check if all classes have the required number of samples\n",
        "def all_classes_completed(class_count, class_dict):\n",
        "    return all(class_count[class_num] == class_dict[class_num][\"total_samples\"] for class_num in class_dict.keys())\n",
        "\n",
        "while not all_classes_completed(class_count, class_dict):\n",
        "    # Loop through the ground truth data\n",
        "    for label in class_dict.keys():\n",
        "        # Get the coordinates of the ground truth pixels\n",
        "        coords = np.argwhere((PaviaU_gt == label))\n",
        "\n",
        "        # Shuffle the coordinates to randomize the patch extraction\n",
        "        np.random.shuffle(coords)\n",
        "\n",
        "        for coord in coords:\n",
        "            i, j = coord\n",
        "            # Calculate the patch indices\n",
        "            i_start, i_end = i - patch_size // 2, i + patch_size // 2 + 1\n",
        "            j_start, j_end = j - patch_size // 2, j + patch_size // 2 + 1\n",
        "\n",
        "            # Check if the indices are within the bounds of the HSI data\n",
        "            if i_start >= 0 and i_end <= PaviaU_hsi.shape[0] and j_start >= 0 and j_end <= PaviaU_hsi.shape[1]:\n",
        "                # Extract the patch\n",
        "                hsi_patch = PaviaU_hsi[i_start:i_end, j_start:j_end, :]\n",
        "\n",
        "                # If the class count is less than the required samples\n",
        "                if class_count[label] < class_dict[label][\"total_samples\"]:\n",
        "                    # Append the patch and its label to the list\n",
        "                    hsi_samples.append(hsi_patch)\n",
        "                    labels.append(label)\n",
        "                    class_count[label] += 1\n",
        "\n",
        "                    # If all classes have the required number of samples, exit the loop\n",
        "                    if all_classes_completed(class_count, class_dict):\n",
        "                        break\n",
        "\n",
        "# Convert the list of patches and labels into arrays\n",
        "hsi_samples = np.array(hsi_samples)\n",
        "labels = np.array(labels)\n",
        "print('hsi_samples shape:', hsi_samples.shape)\n",
        "print('labels shape:', labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXsY3DWSbifj",
        "outputId": "fc59977f-cce6-43e0-a508-2ae7e13b3765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training_hsi_samples shape: (3921, 7, 7, 103)\n",
            "training_labels shape: (3921,)\n",
            "test_hsi_samples shape: (37451, 7, 7, 103)\n",
            "test_labels shape: (37451,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example assumptions, replace with your actual data setup\n",
        "patch_size = 7\n",
        "\n",
        "# Reset class count trackers for a fresh start\n",
        "class_count_training = {i: 0 for i in class_dict.keys()}\n",
        "class_count_test = {i: 0 for i in class_dict.keys()}\n",
        "\n",
        "hsi_samples_training = []\n",
        "labels_training = []\n",
        "hsi_samples_test = []\n",
        "labels_test = []\n",
        "\n",
        "# Let's simplify the check to ensure we're capturing the essence of the loop correctly\n",
        "for label in class_dict.keys():\n",
        "    coords = np.argwhere((PaviaU_gt == label))\n",
        "    np.random.shuffle(coords)\n",
        "\n",
        "    for coord in coords:\n",
        "        if class_count_training[label] >= class_dict[label][\"training_samples\"] and class_count_test[label] >= class_dict[label][\"test_samples\"]:\n",
        "            continue  # Move to the next class if both training and test samples met\n",
        "\n",
        "        i, j = coord\n",
        "        i_start, i_end = i - patch_size // 2, i + patch_size // 2 + 1\n",
        "        j_start, j_end = j - patch_size // 2, j + patch_size // 2 + 1\n",
        "\n",
        "        if i_start >= 0 and i_end <= PaviaU_hsi.shape[0] and j_start >= 0 and j_end <= PaviaU_hsi.shape[1]:\n",
        "            hsi_patch = PaviaU_hsi[i_start:i_end, j_start:j_end, :]\n",
        "\n",
        "            # Decide whether to append to training or test\n",
        "            if class_count_training[label] < class_dict[label][\"training_samples\"]:\n",
        "                hsi_samples_training.append(hsi_patch)\n",
        "                labels_training.append(label)\n",
        "                class_count_training[label] += 1\n",
        "            elif class_count_test[label] < class_dict[label][\"test_samples\"]:\n",
        "                hsi_samples_test.append(hsi_patch)\n",
        "                labels_test.append(label)\n",
        "                class_count_test[label] += 1\n",
        "\n",
        "# Convert lists to arrays\n",
        "training_hsi_samples = np.array(hsi_samples_training)\n",
        "training_labels = np.array(labels_training)\n",
        "test_hsi_samples = np.array(hsi_samples_test)\n",
        "test_labels = np.array(labels_test)\n",
        "\n",
        "print('training_hsi_samples shape:', training_hsi_samples.shape)\n",
        "print('training_labels shape:', training_labels.shape)\n",
        "print('test_hsi_samples shape:', test_hsi_samples.shape)\n",
        "print('test_labels shape:', test_labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQfo01tdWj1b"
      },
      "outputs": [],
      "source": [
        "hsi_train=training_hsi_samples\n",
        "y_train=training_labels\n",
        "hsi_test=test_hsi_samples\n",
        "y_test=test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIUw3YmRWhFi",
        "outputId": "7f0f8047-29f2-414b-dcfc-90da51884de9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Augmented HSI training samples shape: (23526, 7, 7, 103)\n",
            "Augmented training labels shape: (23526,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import rotate\n",
        "\n",
        "def augment_training_data(hsi_training_data,  training_labels, rotations=[45, 90, 135], flip_up_down=True, flip_left_right=True):\n",
        "    augmented_hsi = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    for hsi,label in zip(hsi_training_data,  training_labels):\n",
        "        # Original data\n",
        "        augmented_hsi.append(hsi)\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "        # Rotations\n",
        "        for angle in rotations:\n",
        "            hsi_rotated = rotate(hsi, angle, axes=(0, 1), reshape=False, mode='nearest')\n",
        "            augmented_hsi.append(hsi_rotated)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "        # Flip up-down\n",
        "        if flip_up_down:\n",
        "            hsi_flipped_ud = np.flipud(hsi)\n",
        "\n",
        "            augmented_hsi.append(hsi_flipped_ud)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "        # Flip left-right\n",
        "        if flip_left_right:\n",
        "            hsi_flipped_lr = np.fliplr(hsi)\n",
        "\n",
        "            augmented_hsi.append(hsi_flipped_lr)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "    return np.array(augmented_hsi), np.array(augmented_labels)\n",
        "\n",
        "# Augmenting the training samples\n",
        "augmented_hsi_training_samples,  augmented_training_labels = augment_training_data(hsi_train, y_train)\n",
        "\n",
        "# Print shapes to verify the augmented training data\n",
        "print('Augmented HSI training samples shape:', augmented_hsi_training_samples.shape)\n",
        "print('Augmented training labels shape:', augmented_training_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vVIqLhgvnQj"
      },
      "source": [
        "# 0.0 YAML\n",
        "an overall architecture description in YAML format for a model adapted for hyperspectral image classification that includes patch embedding, spectral band processing, a bidirectional state space model block, and spatial feature processing:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivt40duEwqsP"
      },
      "source": [
        "#1.0  Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6TlJRoRw0z4"
      },
      "outputs": [],
      "source": [
        "# Configuration class\n",
        "class Config:\n",
        "    def __init__(self, in_channels, num_patches, kernel_size, patch_size, emb_size, dim, depth, heads, dim_head, mlp_ratio, num_classes, dropout, pos_emb_size, class_emb_size, stride, output_dim):  # Set default output_dim to 1\n",
        "        self.in_channels = in_channels\n",
        "        self.num_patches = num_patches\n",
        "        self.kernel_size = kernel_size\n",
        "        self.patch_size = patch_size\n",
        "        self.emb_size = emb_size\n",
        "        self.dim = dim\n",
        "        self.depth = depth\n",
        "        self.heads = heads\n",
        "        self.dim_head = dim_head\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout = dropout\n",
        "        self.pos_emb_size = pos_emb_size\n",
        "        self.class_emb_size = class_emb_size\n",
        "        self.stride = stride\n",
        "        self.output_dim = output_dim  # Ensure output_dim is a part of the config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSK_5LFd2Oom"
      },
      "source": [
        "To incorporate the improvement points into the SpectralBandProcessing class for enhanced spectral band processing, including the use of attention mechanisms and other suggested improvements, the class can be extended with a spectral attention layer. This will allow the model to focus on the most informative spectral bands dynamically. Here is how we can integrate these improvements into the existing architecture:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 2.0 This involves reversing the input tensor for the backward path before applying the backward_conv1d operation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class HSIVimBlock(nn.Module):\n",
        "    def __init__(self, spatial_dim, num_bands, hidden_dim, output_dim, delta_param_init):\n",
        "        super(HSIVimBlock, self).__init__()\n",
        "        # Initialization with self.hidden_dim\n",
        "        self.spatial_dim = spatial_dim\n",
        "        self.num_bands = num_bands\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # LayerNorm is now expecting a flattened feature vector of Bands*H*W elements\n",
        "        self.norm = nn.LayerNorm(num_bands * spatial_dim * spatial_dim)\n",
        "\n",
        "        # Adjust linear layers according to the new input dimension\n",
        "        self.linear_x = nn.Linear(num_bands * spatial_dim * spatial_dim, hidden_dim)\n",
        "        self.linear_z = nn.Linear(num_bands * spatial_dim * spatial_dim, hidden_dim)\n",
        "\n",
        "        self.forward_conv1d = nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
        "        self.backward_conv1d = nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
        "\n",
        "        self.A = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "        self.B = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "        #self.C = nn.Parameter(torch.randn(output_dim, hidden_dim))\n",
        "        self.delta_param = nn.Parameter(torch.full((hidden_dim,), delta_param_init))\n",
        "\n",
        "        self.linear_forward = nn.Linear(hidden_dim, output_dim)\n",
        "        self.linear_backward = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Batch, H, W, Bands = x.shape  # Correct shape extraction assuming [Batch, Height, Width, Bands]\n",
        "\n",
        "        # Correctly reshape for LayerNorm to flatten all spatial and spectral information\n",
        "        x = x.reshape(Batch, -1)  # New shape: [Batch, Bands*H*W]\n",
        "\n",
        "        # Normalize across the flattened spatial-spectral data\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Projection to hidden dimensions\n",
        "        x_proj = self.linear_x(x)\n",
        "        z_proj = self.linear_z(x)\n",
        "\n",
        "        # Ensure correct reshaping for Conv1d compatibility\n",
        "        x_proj = x_proj.view(Batch, self.hidden_dim, -1)\n",
        "        z_proj = z_proj.view(Batch, self.hidden_dim, -1)\n",
        "\n",
        "        # Reverse z_proj for the backward path\n",
        "        z_proj_reversed = torch.flip(z_proj, dims=[-1])\n",
        "\n",
        "        # Bidirectional Conv1d processing using reversed input for the backward path\n",
        "        x_forward = F.silu(self.forward_conv1d(x_proj))\n",
        "        x_backward = F.silu(self.backward_conv1d(z_proj_reversed))\n",
        "\n",
        "        # Apply delta parameter correctly\n",
        "        delta_expanded = self.delta_param.unsqueeze(0).unsqueeze(2)  # Correct shape for broadcasting\n",
        "\n",
        "        # SSM processing with delta applied, using the original and reversed inputs for forward and backward paths respectively\n",
        "        forward_ssm_output = torch.tanh(self.forward_conv1d(x_proj) + self.A * delta_expanded)\n",
        "        backward_ssm_output = torch.tanh(self.backward_conv1d(z_proj_reversed) + self.B * delta_expanded)\n",
        "\n",
        "        # Combine forward and backward outputs into a single representation\n",
        "        forward_reduced = forward_ssm_output.mean(dim=2)\n",
        "        backward_reduced = backward_ssm_output.mean(dim=2)\n",
        "\n",
        "        # Combine the reduced forward and backward paths\n",
        "        y_forward = self.linear_forward(forward_reduced)\n",
        "        y_backward = self.linear_backward(backward_reduced)\n",
        "\n",
        "        # Element-wise sum of forward and backward outputs\n",
        "        y_combined = y_forward + y_backward\n",
        "\n",
        "        # Return the combined output\n",
        "        return y_combined"
      ],
      "metadata": {
        "id": "wBo3j8MyYkKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02uU_S6M3EQf"
      },
      "outputs": [],
      "source": [
        "# New version\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SpatialFeatureProcessing(nn.Module):\n",
        "    def __init__(self, input_channels):\n",
        "        super(SpatialFeatureProcessing, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            # First convolutional layer with dilation rate of 1 (standard convolution)\n",
        "            nn.Conv2d(in_channels=input_channels, out_channels=256, kernel_size=(3, 3), padding=1, dilation=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            # Second convolutional layer with a higher dilation rate to increase the receptive field\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3, 3), padding=2, dilation=2),  # Note the increased padding to maintain the spatial dimensions\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512)\n",
        "        )\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Adding global average pooling\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.global_avg_pool(x)  # Apply global average pooling\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten all dimensions except batch\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCVCyDOHf0ol"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_features, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(in_features=in_features, out_features=1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=1024, out_features=num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc_layers(x)\n",
        "        # Remove softmax here if you're using a loss function that includes it, such as nn.CrossEntropyLoss\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-iDoAegAj8D"
      },
      "source": [
        "###1.4Integrated into Main Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbvWBj6CjoQm"
      },
      "outputs": [],
      "source": [
        "class HSIClassificationMambaModel(nn.Module):\n",
        "    def __init__(self, spatial_dim, num_bands, hidden_dim, output_dim, delta_param_init, num_classes):\n",
        "        super(HSIClassificationMambaModel, self).__init__()\n",
        "        self.vim_block = HSIVimBlock(spatial_dim, num_bands, hidden_dim, output_dim, delta_param_init)\n",
        "        self.output_dim = output_dim  # Save output_dim as an attribute of the class\n",
        "\n",
        "        # Initialize SpatialFeatureProcessing and Classifier here\n",
        "        # Adjusted to pass 'output_dim' as 'input_channels' to SpatialFeatureProcessing\n",
        "        self.spatial_processing = SpatialFeatureProcessing(input_channels=output_dim)\n",
        "        # Assuming the output of SpatialFeatureProcessing matches the in_features expected by Classifier\n",
        "        self.classifier = Classifier(in_features=512, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vim_block(x)\n",
        "        # This is a placeholder. Actual reshaping depends on the output of HSIVimBlock and the input expectation of SpatialFeatureProcessing\n",
        "        x = x.view(-1, self.output_dim, 1, 1)  # Reshape to include spatial dimensions if needed\n",
        "        x = self.spatial_processing(x)\n",
        "\n",
        "        # Flatten the output from spatial processing if it's not already flat\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YPTp6kBEwgr"
      },
      "source": [
        "# Instance the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjaOisQ-e9vP",
        "outputId": "5faec6bf-438f-4132-af88-192293dc6bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HSIClassificationMambaModel(\n",
            "  (vim_block): HSIVimBlock(\n",
            "    (norm): LayerNorm((5047,), eps=1e-05, elementwise_affine=True)\n",
            "    (linear_x): Linear(in_features=5047, out_features=256, bias=True)\n",
            "    (linear_z): Linear(in_features=5047, out_features=256, bias=True)\n",
            "    (forward_conv1d): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    (backward_conv1d): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    (linear_forward): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (linear_backward): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (spatial_processing): SpatialFeatureProcessing(\n",
            "    (conv_layers): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
            "      (4): ReLU()\n",
            "      (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (classifier): Classifier(\n",
            "    (fc_layers): Sequential(\n",
            "      (0): Linear(in_features=512, out_features=1024, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "      (3): Linear(in_features=1024, out_features=9, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Instantiate the model\n",
        "model = HSIClassificationMambaModel(\n",
        "    spatial_dim=7,\n",
        "    num_bands=103,\n",
        "    hidden_dim=256,\n",
        "    output_dim=128,  # Make sure this matches the actual output of your HSIVimBlock\n",
        "    delta_param_init=0.1,\n",
        "    num_classes=9\n",
        ")\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7hfU1LYg_PW"
      },
      "source": [
        "### Training Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aH8AYC73RUK",
        "outputId": "5c20d75e-aa33-4af4-9a13-0a17efe237df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (21173, 7, 7, 103)\n",
            "X_train_val shape: (2353, 7, 7, 103)\n",
            "y_train shape: (21173,)\n",
            "X_test shape: (37451, 7, 7, 103)\n",
            "y_test shape: (37451,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the augmented training data into training, validationsets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    augmented_hsi_training_samples, augmented_training_labels, test_size=0.1, random_state=42, stratify=augmented_training_labels\n",
        ")\n",
        "X_test=hsi_test\n",
        "y_test=y_test\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_train_val shape:', X_val.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "print('X_test shape:', X_test.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "\n",
        "\n",
        "# Convert the splitted datasets to tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train.astype(np.float32)), torch.tensor(y_train).long())\n",
        "val_dataset = TensorDataset(torch.tensor(X_val.astype(np.float32)), torch.tensor(y_val).long())\n",
        "test_dataset = TensorDataset(torch.tensor(X_test.astype(np.float32)), torch.tensor(y_test).long())\n",
        "\n",
        "# Create DataLoader instances for training, validation, and testing\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLSx9YAVyofn"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.utils.data import DataLoader, random_split\n",
        "# import numpy as np\n",
        "\n",
        "# batch_size = 32\n",
        "\n",
        "# # Training DataLoader\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # Validation DataLoader\n",
        "# val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# # Testing DataLoader\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YKbJ8f9kYbF"
      },
      "source": [
        "# 5.0 Training Model Memeory and Time calcualtion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3VFoCtttkXQL"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "import os\n",
        "\n",
        "# Function to get current process memory usage\n",
        "def get_memory_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / (1024 * 1024)  # Convert bytes to MB\n",
        "\n",
        "initial_memory = get_memory_usage()\n",
        "print(f\"Initial Memory Usage: {initial_memory:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhsKBbhgvrUy"
      },
      "outputs": [],
      "source": [
        "# # Before the training loop, to record the initial memory usage (GPU)\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.reset_peak_memory_stats()  # Reset peak memory stats at the start\n",
        "#     initial_memory = torch.cuda.memory_allocated()\n",
        "#     print(f\"Initial Memory Allocated: {initial_memory / 1e6} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtM-XdHTizW-"
      },
      "source": [
        "### 5.1 Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVwvqHib6Mvy",
        "outputId": "5edbd0b7-36cb-44db-f08a-da79e63d70ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Train Loss: 0.0310, Val Loss: 0.0176\n",
            "Validation Loss Decreased(inf--->0.017630) \t Saving The Model\n",
            "Epoch [2/50], Train Loss: 0.0151, Val Loss: 0.0116\n",
            "Validation Loss Decreased(0.017630--->0.011589) \t Saving The Model\n",
            "Epoch [3/50], Train Loss: 0.0101, Val Loss: 0.0094\n",
            "Validation Loss Decreased(0.011589--->0.009418) \t Saving The Model\n",
            "Epoch [4/50], Train Loss: 0.0075, Val Loss: 0.0074\n",
            "Validation Loss Decreased(0.009418--->0.007401) \t Saving The Model\n",
            "Epoch [5/50], Train Loss: 0.0060, Val Loss: 0.0076\n",
            "Epoch [6/50], Train Loss: 0.0049, Val Loss: 0.0046\n",
            "Validation Loss Decreased(0.007401--->0.004635) \t Saving The Model\n",
            "Epoch [7/50], Train Loss: 0.0043, Val Loss: 0.0053\n",
            "Epoch [8/50], Train Loss: 0.0037, Val Loss: 0.0036\n",
            "Validation Loss Decreased(0.004635--->0.003622) \t Saving The Model\n",
            "Epoch [9/50], Train Loss: 0.0033, Val Loss: 0.0031\n",
            "Validation Loss Decreased(0.003622--->0.003075) \t Saving The Model\n",
            "Epoch [10/50], Train Loss: 0.0029, Val Loss: 0.0059\n",
            "Epoch [11/50], Train Loss: 0.0026, Val Loss: 0.0022\n",
            "Validation Loss Decreased(0.003075--->0.002168) \t Saving The Model\n",
            "Epoch [12/50], Train Loss: 0.0025, Val Loss: 0.0033\n",
            "Epoch [13/50], Train Loss: 0.0022, Val Loss: 0.0034\n",
            "Epoch [14/50], Train Loss: 0.0019, Val Loss: 0.0029\n",
            "Epoch [15/50], Train Loss: 0.0020, Val Loss: 0.0124\n",
            "Epoch [16/50], Train Loss: 0.0019, Val Loss: 0.0035\n",
            "Epoch [17/50], Train Loss: 0.0017, Val Loss: 0.0031\n",
            "Epoch [18/50], Train Loss: 0.0015, Val Loss: 0.0067\n",
            "Epoch [19/50], Train Loss: 0.0015, Val Loss: 0.0020\n",
            "Validation Loss Decreased(0.002168--->0.002046) \t Saving The Model\n",
            "Epoch [20/50], Train Loss: 0.0015, Val Loss: 0.0029\n",
            "Epoch [21/50], Train Loss: 0.0014, Val Loss: 0.0048\n",
            "Epoch [22/50], Train Loss: 0.0013, Val Loss: 0.0022\n",
            "Epoch [23/50], Train Loss: 0.0012, Val Loss: 0.0062\n",
            "Epoch [24/50], Train Loss: 0.0011, Val Loss: 0.0038\n",
            "Epoch [25/50], Train Loss: 0.0011, Val Loss: 0.0020\n",
            "Validation Loss Decreased(0.002046--->0.001994) \t Saving The Model\n",
            "Epoch [26/50], Train Loss: 0.0011, Val Loss: 0.0022\n",
            "Epoch [27/50], Train Loss: 0.0011, Val Loss: 0.0067\n",
            "Epoch [28/50], Train Loss: 0.0010, Val Loss: 0.0015\n",
            "Validation Loss Decreased(0.001994--->0.001529) \t Saving The Model\n",
            "Epoch [29/50], Train Loss: 0.0010, Val Loss: 0.0033\n",
            "Epoch [30/50], Train Loss: 0.0009, Val Loss: 0.0022\n",
            "Epoch [31/50], Train Loss: 0.0009, Val Loss: 0.0016\n",
            "Epoch [32/50], Train Loss: 0.0009, Val Loss: 0.0037\n",
            "Epoch [33/50], Train Loss: 0.0008, Val Loss: 0.0020\n",
            "Epoch [34/50], Train Loss: 0.0008, Val Loss: 0.0017\n",
            "Epoch [35/50], Train Loss: 0.0008, Val Loss: 0.0036\n",
            "Epoch [36/50], Train Loss: 0.0008, Val Loss: 0.0019\n",
            "Epoch [37/50], Train Loss: 0.0008, Val Loss: 0.0020\n",
            "Epoch [38/50], Train Loss: 0.0007, Val Loss: 0.0025\n",
            "Epoch [39/50], Train Loss: 0.0009, Val Loss: 0.0019\n",
            "Early stopping!\n",
            "Finished training. Total training time: 4929.51 seconds\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "import time  # Step 1: Import the time module\n",
        "\n",
        "model = HSIClassificationMambaModel(\n",
        "    spatial_dim=7, num_bands=103, hidden_dim=256, output_dim=128, delta_param_init=0.01, num_classes=9\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "\n",
        "epochs = 50\n",
        "best_val_loss = float('inf')\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "patience = 10\n",
        "\n",
        "start_time = time.time()  # Step 2: Record the start time\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        labels -= 1\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            labels -= 1\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_running_loss += loss.item()\n",
        "\n",
        "        epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
        "\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        print(f'Validation Loss Decreased({best_val_loss:.6f}--->{epoch_val_loss:.6f}) \\t Saving The Model')\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        no_improve_epochs = 0\n",
        "    else:\n",
        "        no_improve_epochs += 1\n",
        "\n",
        "    if no_improve_epochs > patience:\n",
        "        print('Early stopping!')\n",
        "        model.load_state_dict(best_model_wts)\n",
        "        break\n",
        "\n",
        "end_time = time.time()  # Step 3: Record the end time\n",
        "total_time = end_time - start_time  # Step 4: Calculate total training time\n",
        "\n",
        "print(f'Finished training. Total training time: {total_time:.2f} seconds')  # Print the total training time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjU6eWgo8wEN"
      },
      "source": [
        "### We kow we use GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjHq_LfmkrBk"
      },
      "outputs": [],
      "source": [
        "# # Memory usage clacualtion\n",
        "# final_memory = get_memory_usage()\n",
        "# print(f\"Final Memory Usage: {final_memory:.2f} MB\")\n",
        "\n",
        "# memory_used = final_memory - initial_memory\n",
        "# print(f\"Memory Used: {memory_used:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VC9h8grt9Kx_"
      },
      "outputs": [],
      "source": [
        "## If GPU USed\n",
        "# if torch.cuda.is_available():\n",
        "#     final_memory = torch.cuda.memory_allocated()\n",
        "#     peak_memory = torch.cuda.max_memory_allocated()\n",
        "#     print(f\"Final Memory Allocated: {final_memory / 1e6} MB\")\n",
        "#     print(f\"Peak Memory Allocated During Training: {peak_memory / 1e6} MB\")\n",
        "#     memory_used = final_memory - initial_memory\n",
        "#     print(f\"Memory Used: {memory_used / 1e6} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljUEozyPnONL"
      },
      "source": [
        "### Save the modle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp56-lp2nQcJ"
      },
      "outputs": [],
      "source": [
        "# Assuming 'model' is your instance of HSIClassificationModel or any other model\n",
        "# and it's been trained\n",
        "torch.save(model.state_dict(),'PaviaU_P7GPU_ab_model_state_dict.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nUaBmik7SFf"
      },
      "source": [
        "### Claculte th test time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2TLXgdj7VxF",
        "outputId": "f7d3d68b-aa1e-438b-ed1d-ff33d64392da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to PaviaU_P7GPU_ab_model_state_dict.pth\n",
            "Overall Accuracy (OA): 0.9675\n",
            "Average Accuracy (AA): 0.9701\n",
            "Kappa Coefficient: 0.9563\n",
            "Test time: 3.43 seconds\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
        "import time  # Import the time module for timing the test phase\n",
        "\n",
        "# Assuming 'model' is your instance of HSIClassificationModel or any other model\n",
        "# and it's been trained\n",
        "\n",
        "# Save the model\n",
        "model_save_path =  'PaviaU_P7GPU_ab_model_state_dict.pth'\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f'Model saved to {model_save_path}')\n",
        "\n",
        "# Load the model (make sure to initialize the model architecture first)\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "model.to(device)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Store predictions and actual labels\n",
        "predictions = []\n",
        "actual_labels = []\n",
        "\n",
        "start_time = time.time()  # Start timing\n",
        "\n",
        "with torch.no_grad():\n",
        "    for hsi_patches, labels in test_loader:\n",
        "        # Move data to the appropriate device\n",
        "        hsi_patches = hsi_patches.to(device)\n",
        "        labels -= 1  # Adjust labels if necessary\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(hsi_patches)\n",
        "\n",
        "        # Get predictions\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "        actual_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "end_time = time.time()  # End timing\n",
        "test_time = end_time - start_time  # Calculate the test time\n",
        "\n",
        "# Optionally, calculate accuracy or other metrics using predictions and actual_labels\n",
        "\n",
        "# Convert lists to NumPy arrays for easier manipulation\n",
        "predictions_array = np.array(predictions)\n",
        "actual_labels_array = np.array(actual_labels)\n",
        "\n",
        "# Overall Accuracy\n",
        "oa = accuracy_score(actual_labels_array, predictions_array)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(actual_labels_array, predictions_array)\n",
        "# Calculate per-class accuracy from the confusion matrix\n",
        "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
        "# Average Accuracy\n",
        "aa = np.mean(class_accuracy)\n",
        "\n",
        "# Kappa Coefficient\n",
        "kappa = cohen_kappa_score(actual_labels_array, predictions_array)\n",
        "\n",
        "print(f'Overall Accuracy (OA): {oa:.4f}')\n",
        "print(f'Average Accuracy (AA): {aa:.4f}')\n",
        "print(f'Kappa Coefficient: {kappa:.4f}')\n",
        "print(f'Test time: {test_time:.2f} seconds')  # Print the test time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSNizq7OCBFW",
        "outputId": "b61ed049-8a00-4ad0-f769-a79d38e27668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class 1 Accuracy: 0.9702\n",
            "Class 2 Accuracy: 0.9686\n",
            "Class 3 Accuracy: 0.9477\n",
            "Class 4 Accuracy: 0.9908\n",
            "Class 5 Accuracy: 1.0000\n",
            "Class 6 Accuracy: 0.9789\n",
            "Class 7 Accuracy: 0.9801\n",
            "Class 8 Accuracy: 0.9141\n",
            "Class 9 Accuracy: 0.9804\n"
          ]
        }
      ],
      "source": [
        "for i, acc in enumerate(class_accuracy): print(f'Class {i+1} Accuracy: {acc:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}